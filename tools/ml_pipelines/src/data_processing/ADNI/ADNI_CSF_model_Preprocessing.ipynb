{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f248c735"
      },
      "source": [
        "## ADNIMERGE Data Preprocessing - Baseline Data Cleaning\n",
        "\n",
        "Objective: Clean and prepare the baseline data from the ADNIMERGE dataset for downstream analysis, specifically CSF prediction.\n",
        "\n",
        "Input Data:\n",
        "\n",
        "ADNIMERGE dataset (`ADNIMERGE_25Apr2025.csv`) containing clinical, demographic, biomarker, and imaging data.\n",
        "\n",
        "Processing Steps:\n",
        "\n",
        "1.  **Load Raw Data**: Load the raw ADNIMERGE dataset into a pandas DataFrame.\n",
        "2.  **Select Baseline Data**: Filter the DataFrame to include only baseline measurements and key identifiers.\n",
        "3.  **Drop Ecog Scores**: Remove columns related to Ecog scores.\n",
        "4.  **Deduplicate**: Remove duplicate rows and keep the first entry for each unique participant ID (`PTID`).\n",
        "5.  **Handle High Missingness**: Identify and drop columns with more than 50% missing values.\n",
        "6.  **Impute Missing Values**: Apply mean imputation to continuous variables and most frequent imputation to categorical variables.\n",
        "\n",
        "Output:\n",
        "\n",
        "Cleaned baseline dataset (`df_clean.csv`) with reduced missingness and a unique entry per participant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69785ec8"
      },
      "source": [
        "**Reasoning**:\n",
        "The goal is to create a function to load and filter the baseline data. This involves defining the function, reading the CSV, selecting columns, and returning the filtered DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d227ff4b"
      },
      "source": [
        "def load_and_filter_baseline_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads the ADNIMERGE data from a CSV file and selects a predefined set of\n",
        "    baseline columns.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the ADNIMERGE CSV file.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: A DataFrame containing only the baseline columns.\n",
        "    \"\"\"\n",
        "    # Handle potential mixed types based on previous warning\n",
        "    df = pd.read_csv(file_path, low_memory=False)\n",
        "\n",
        "    baseline_cols = [\n",
        "        'RID', 'PTID', 'DX_bl', 'AGE', 'PTGENDER', 'PTEDUCAT', 'APOE4',\n",
        "        # Biomarkers\n",
        "        'ABETA_bl', 'TAU_bl', 'PTAU_bl', 'FDG_bl', 'PIB_bl', 'AV45_bl', 'FBB_bl',\n",
        "        # Clinical scores\n",
        "        'CDRSB_bl', 'ADAS11_bl', 'ADAS13_bl', 'ADASQ4_bl', 'MMSE_bl',\n",
        "        'RAVLT_immediate_bl', 'RAVLT_learning_bl', 'RAVLT_forgetting_bl', 'RAVLT_perc_forgetting_bl',\n",
        "        'LDELTOTAL_BL', 'DIGITSCOR_bl', 'TRABSCOR_bl', 'FAQ_bl',\n",
        "        'MOCA_bl', 'EcogPtMem_bl', 'EcogPtLang_bl', 'EcogPtVisspat_bl',\n",
        "        'EcogPtPlan_bl', 'EcogPtOrgan_bl', 'EcogPtDivatt_bl', 'EcogPtTotal_bl',\n",
        "        'EcogSPMem_bl', 'EcogSPLang_bl', 'EcogSPVisspat_bl', 'EcogSPPlan_bl',\n",
        "        'EcogSPOrgan_bl', 'EcogSPDivatt_bl', 'EcogSPTotal_bl',\n",
        "        # MRI volumes\n",
        "        'Ventricles_bl', 'Hippocampus_bl', 'WholeBrain_bl', 'Entorhinal_bl',\n",
        "        'Fusiform_bl', 'MidTemp_bl', 'ICV_bl'\n",
        "    ]\n",
        "\n",
        "    df_bl = df[baseline_cols]\n",
        "\n",
        "    return df_bl\n",
        "\n",
        "# Example usage (assuming the file is in the same directory)\n",
        "# df_bl = load_and_filter_baseline_data(\"ADNIMERGE_25Apr2025.csv\")\n",
        "# display(df_bl.head())"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "639ec0c9"
      },
      "source": [
        "## Refactor column dropping\n",
        "\n",
        "### Subtask:\n",
        "Create a function to drop the specified Ecog score columns and the high-missingness columns from the DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ab671f2"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function to drop specified columns from the DataFrame, including Ecog scores and high-missingness features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e196255"
      },
      "source": [
        "def drop_irrelevant_and_high_missingness_columns(df):\n",
        "    \"\"\"\n",
        "    Drops specified Ecog score columns and high-missingness columns from the DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: The DataFrame with specified columns dropped.\n",
        "    \"\"\"\n",
        "    ecog_cols = [\n",
        "        'EcogPtMem_bl', 'EcogPtLang_bl', 'EcogPtVisspat_bl',\n",
        "        'EcogPtPlan_bl', 'EcogPtOrgan_bl', 'EcogPtDivatt_bl', 'EcogPtTotal_bl',\n",
        "        'EcogSPMem_bl', 'EcogSPLang_bl', 'EcogSPVisspat_bl', 'EcogSPPlan_bl',\n",
        "        'EcogSPOrgan_bl', 'EcogSPDivatt_bl', \"EcogSPTotal_bl\",\n",
        "    ]\n",
        "    high_missingness_cols = ['PIB_bl', 'FBB_bl', 'DIGITSCOR_bl', 'AV45_bl']\n",
        "\n",
        "    cols_to_drop = ecog_cols + high_missingness_cols\n",
        "\n",
        "    df_dropped = df.drop(columns=cols_to_drop, errors='ignore') # Use errors='ignore' to avoid issues if a column is already missing\n",
        "\n",
        "    return df_dropped\n",
        "\n",
        "# Example usage (assuming df_bl is the DataFrame after loading and filtering)\n",
        "# df_dropped = drop_irrelevant_and_high_missingness_columns(df_bl)\n",
        "# display(df_dropped.head())\n",
        "# print(df_dropped.shape)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "486381b4"
      },
      "source": [
        "## Refactor deduplication\n",
        "\n",
        "### Subtask:\n",
        "Create a function to handle deduplication of the DataFrame based on the 'PTID' column, keeping the first entry for each unique participant.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c66b8f1a"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function to handle deduplication based on 'PTID'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8762463b"
      },
      "source": [
        "def deduplicate_by_ptid(df):\n",
        "    \"\"\"\n",
        "    Deduplicates the DataFrame based on the 'PTID' column, keeping the first\n",
        "    entry for each unique participant.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: The deduplicated DataFrame with reset index.\n",
        "    \"\"\"\n",
        "    # Remove exact duplicate rows first\n",
        "    df_nodup = df.drop_duplicates()\n",
        "\n",
        "    # Then deduplicate based on PTID, keeping the first entry\n",
        "    df_nodup = df_nodup.drop_duplicates(subset='PTID', keep='first')\n",
        "\n",
        "    # Reset index\n",
        "    df_nodup.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return df_nodup\n",
        "\n",
        "# Example usage (assuming df_bl1 is the DataFrame after dropping columns)\n",
        "# df_bl1_nodup = deduplicate_by_ptid(df_bl1)\n",
        "# print(\"Unique PTIDs after deduplication:\", df_bl1_nodup['PTID'].nunique())\n",
        "# print(\"Shape of cleaned baseline DataFrame:\", df_bl1_nodup.shape)\n",
        "# display(df_bl1_nodup.head())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cade2cf2"
      },
      "source": [
        "## Refactor missing value imputation\n",
        "\n",
        "### Subtask:\n",
        "Create a function to impute missing values in the DataFrame, applying mean imputation to continuous variables and most frequent imputation to categorical variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "331187b5"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function to impute missing values in the DataFrame using mean for continuous and most frequent for categorical columns.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d189d2da"
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "def impute_missing_values(df):\n",
        "    \"\"\"\n",
        "    Imputes missing values in the DataFrame, applying mean imputation to\n",
        "    continuous variables and most frequent imputation to categorical variables.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): The input DataFrame with missing values.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: The DataFrame with missing values imputed.\n",
        "    \"\"\"\n",
        "    # Separate continuous and categorical columns based on previous analysis\n",
        "    continuous_cols = [\n",
        "        'FDG_bl', 'MOCA_bl', 'WholeBrain_bl', 'Entorhinal_bl', 'Fusiform_bl',\n",
        "        'MidTemp_bl', 'ICV_bl', 'AGE','Ventricles_bl', 'Hippocampus_bl',\n",
        "        'PTEDUCAT', 'CDRSB_bl', 'ADAS11_bl', 'ADAS13_bl', 'ADASQ4_bl',\n",
        "        'MMSE_bl', 'RAVLT_immediate_bl', 'RAVLT_learning_bl',\n",
        "        'RAVLT_forgetting_bl', 'RAVLT_perc_forgetting_bl', 'LDELTOTAL_BL',\n",
        "        'TRABSCOR_bl', 'FAQ_bl', 'MOCA_bl'\n",
        "    ]\n",
        "    categorical_cols = ['APOE4', 'DX_bl']\n",
        "\n",
        "    # Mean imputation for continuous\n",
        "    imp_mean = SimpleImputer(strategy='mean')\n",
        "    df[continuous_cols] = imp_mean.fit_transform(df[continuous_cols])\n",
        "\n",
        "    # Most frequent for categorical\n",
        "    imp_freq = SimpleImputer(strategy='most_frequent')\n",
        "    df[categorical_cols] = imp_freq.fit_transform(df[categorical_cols])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example usage (assuming df_clean is the DataFrame after dropping columns and deduplication)\n",
        "# df_imputed = impute_missing_values(df_clean.copy()) # Use a copy to avoid modifying the original DataFrame\n",
        "# missing_percent = df_imputed.isnull().mean() * 100\n",
        "# print(\"\\nMissingness after imputation:\\n\", missing_percent[missing_percent > 0])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dbbdd89"
      },
      "source": [
        "## Integrate functions into a main script\n",
        "\n",
        "### Subtask:\n",
        "Create a main script that calls the refactored functions in the correct order to perform the entire data cleaning and preprocessing pipeline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36eb58d4"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the main function to orchestrate the data cleaning pipeline by calling the previously refactored functions in sequence and save the cleaned data to a CSV file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d79aba23",
        "outputId": "b5420f1c-65ba-4d4e-e13e-6d943c6a028f"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer # Import SimpleImputer here as it's used in impute_missing_values\n",
        "\n",
        "def load_and_filter_baseline_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads the ADNIMERGE data from a CSV file and selects a predefined set of\n",
        "    baseline columns.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the ADNIMERGE CSV file.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: A DataFrame containing only the baseline columns.\n",
        "    \"\"\"\n",
        "    # Handle potential mixed types based on previous warning\n",
        "    df = pd.read_csv(file_path, low_memory=False)\n",
        "\n",
        "    baseline_cols = [\n",
        "        'RID', 'PTID', 'DX_bl', 'AGE', 'PTGENDER', 'PTEDUCAT', 'APOE4',\n",
        "        # Biomarkers\n",
        "        'ABETA_bl', 'TAU_bl', 'PTAU_bl', 'FDG_bl', 'PIB_bl', 'AV45_bl', 'FBB_bl',\n",
        "        # Clinical scores\n",
        "        'CDRSB_bl', 'ADAS11_bl', 'ADAS13_bl', 'ADASQ4_bl', 'MMSE_bl',\n",
        "        'RAVLT_immediate_bl', 'RAVLT_learning_bl', 'RAVLT_forgetting_bl', 'RAVLT_perc_forgetting_bl',\n",
        "        'LDELTOTAL_BL', 'DIGITSCOR_bl', 'TRABSCOR_bl', 'FAQ_bl',\n",
        "        'MOCA_bl', 'EcogPtMem_bl', 'EcogPtLang_bl', 'EcogPtVisspat_bl',\n",
        "        'EcogPtPlan_bl', 'EcogPtOrgan_bl', 'EcogPtDivatt_bl', 'EcogPtTotal_bl',\n",
        "        'EcogSPMem_bl', 'EcogSPLang_bl', 'EcogSPVisspat_bl', 'EcogSPPlan_bl',\n",
        "        'EcogSPOrgan_bl', 'EcogSPDivatt_bl', 'EcogSPTotal_bl',\n",
        "        # MRI volumes\n",
        "        'Ventricles_bl', 'Hippocampus_bl', 'WholeBrain_bl', 'Entorhinal_bl',\n",
        "        'Fusiform_bl', 'MidTemp_bl', 'ICV_bl'\n",
        "    ]\n",
        "\n",
        "    df_bl = df[baseline_cols]\n",
        "\n",
        "    return df_bl\n",
        "\n",
        "def drop_irrelevant_and_high_missingness_columns(df):\n",
        "    \"\"\"\n",
        "    Drops specified Ecog score columns and high-missingness columns from the DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: The DataFrame with specified columns dropped.\n",
        "    \"\"\"\n",
        "    ecog_cols = [\n",
        "        'EcogPtMem_bl', 'EcogPtLang_bl', 'EcogPtVisspat_bl',\n",
        "        'EcogPtPlan_bl', 'EcogPtOrgan_bl', 'EcogPtDivatt_bl', 'EcogPtTotal_bl',\n",
        "        'EcogSPMem_bl', 'EcogSPLang_bl', 'EcogSPVisspat_bl', 'EcogSPPlan_bl',\n",
        "        'EcogSPOrgan_bl', 'EcogSPDivatt_bl', \"EcogSPTotal_bl\",\n",
        "    ]\n",
        "    high_missingness_cols = ['PIB_bl', 'FBB_bl', 'DIGITSCOR_bl', 'AV45_bl']\n",
        "\n",
        "    cols_to_drop = ecog_cols + high_missingness_cols\n",
        "\n",
        "    df_dropped = df.drop(columns=cols_to_drop, errors='ignore') # Use errors='ignore' to avoid issues if a column is already missing\n",
        "\n",
        "    return df_dropped\n",
        "\n",
        "def deduplicate_by_ptid(df):\n",
        "    \"\"\"\n",
        "    Deduplicates the DataFrame based on the 'PTID' column, keeping the first\n",
        "    entry for each unique participant.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: The deduplicated DataFrame with reset index.\n",
        "    \"\"\"\n",
        "    # Remove exact duplicate rows first\n",
        "    df_nodup = df.drop_duplicates()\n",
        "\n",
        "    # Then deduplicate based on PTID, keeping the first entry\n",
        "    df_nodup = df_nodup.drop_duplicates(subset='PTID', keep='first')\n",
        "\n",
        "    # Reset index\n",
        "    df_nodup.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return df_nodup\n",
        "\n",
        "def impute_missing_values(df):\n",
        "    \"\"\"\n",
        "    Imputes missing values in the DataFrame, applying mean imputation to\n",
        "    continuous variables and most frequent imputation to categorical variables.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): The input DataFrame with missing values.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: The DataFrame with missing values imputed.\n",
        "    \"\"\"\n",
        "    # Separate continuous and categorical columns based on previous analysis\n",
        "    continuous_cols = [\n",
        "        'FDG_bl', 'MOCA_bl', 'WholeBrain_bl', 'Entorhinal_bl', 'Fusiform_bl',\n",
        "        'MidTemp_bl', 'ICV_bl', 'AGE','Ventricles_bl', 'Hippocampus_bl',\n",
        "        'PTEDUCAT', 'CDRSB_bl', 'ADAS11_bl', 'ADAS13_bl', 'ADASQ4_bl',\n",
        "        'MMSE_bl', 'RAVLT_immediate_bl', 'RAVLT_learning_bl',\n",
        "        'RAVLT_forgetting_bl', 'RAVLT_perc_forgetting_bl', 'LDELTOTAL_BL',\n",
        "        'TRABSCOR_bl', 'FAQ_bl', 'MOCA_bl'\n",
        "    ]\n",
        "    categorical_cols = ['APOE4', 'DX_bl']\n",
        "\n",
        "    # Mean imputation for continuous\n",
        "    imp_mean = SimpleImputer(strategy='mean')\n",
        "    # Only impute if columns exist in the dataframe\n",
        "    existing_continuous_cols = [col for col in continuous_cols if col in df.columns]\n",
        "    if existing_continuous_cols:\n",
        "        df[existing_continuous_cols] = imp_mean.fit_transform(df[existing_continuous_cols])\n",
        "\n",
        "    # Most frequent for categorical\n",
        "    imp_freq = SimpleImputer(strategy='most_frequent')\n",
        "    # Only impute if columns exist in the dataframe\n",
        "    existing_categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
        "    if existing_categorical_cols:\n",
        "        df[existing_categorical_cols] = imp_freq.fit_transform(df[existing_categorical_cols])\n",
        "\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the ADNIMERGE baseline data cleaning pipeline.\n",
        "    \"\"\"\n",
        "    file_path = \"ADNIMERGE_25Apr2025.csv\"\n",
        "    output_file_path = \"df_clean.csv\"\n",
        "\n",
        "    print(f\"Loading data from {file_path} and filtering for baseline...\")\n",
        "    df_bl = load_and_filter_baseline_data(file_path)\n",
        "    print(f\"Initial baseline data shape: {df_bl.shape}\")\n",
        "\n",
        "    print(\"Dropping irrelevant and high-missingness columns...\")\n",
        "    df_dropped = drop_irrelevant_and_high_missingness_columns(df_bl)\n",
        "    print(f\"Shape after dropping columns: {df_dropped.shape}\")\n",
        "\n",
        "    print(\"Deduplicating data by PTID...\")\n",
        "    df_dedup = deduplicate_by_ptid(df_dropped)\n",
        "    print(f\"Shape after deduplication: {df_dedup.shape}\")\n",
        "    print(f\"Unique PTIDs after deduplication: {df_dedup['PTID'].nunique()}\")\n",
        "\n",
        "\n",
        "    print(\"Imputing missing values...\")\n",
        "    df_clean = impute_missing_values(df_dedup.copy()) # Use a copy to avoid modifying the original DataFrame\n",
        "    print(\"Missingness after imputation:\")\n",
        "    missing_percent_after_imputation = df_clean.isnull().mean() * 100\n",
        "    print(missing_percent_after_imputation[missing_percent_after_imputation > 0])\n",
        "\n",
        "\n",
        "    print(f\"Saving cleaned data to {output_file_path}...\")\n",
        "    df_clean.to_csv(output_file_path, index=False)\n",
        "    print(\"Data cleaning pipeline completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from ADNIMERGE_25Apr2025.csv and filtering for baseline...\n",
            "Initial baseline data shape: (16421, 49)\n",
            "Dropping irrelevant and high-missingness columns...\n",
            "Shape after dropping columns: (16421, 31)\n",
            "Deduplicating data by PTID...\n",
            "Shape after deduplication: (2430, 31)\n",
            "Unique PTIDs after deduplication: 2430\n",
            "Imputing missing values...\n",
            "Missingness after imputation:\n",
            "ABETA_bl    50.0\n",
            "TAU_bl      50.0\n",
            "PTAU_bl     50.0\n",
            "dtype: float64\n",
            "Saving cleaned data to df_clean.csv...\n",
            "Data cleaning pipeline completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5efa4c94"
      },
      "source": [
        "## Add comments and docstrings\n",
        "\n",
        "### Subtask:\n",
        "Add comments to explain each step and function, and add docstrings to the functions in the main script.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ba05891"
      },
      "source": [
        "**Reasoning**:\n",
        "Add comments and docstrings to the main script to explain each step and function, fulfilling the subtask requirement.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d23a5bab"
      },
      "source": [
        "## Save the cleaned data\n",
        "\n",
        "### Subtask:\n",
        "Save the cleaned DataFrame to a CSV file named 'df_clean.csv' without including the DataFrame index.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4982368"
      },
      "source": [
        "**Reasoning**:\n",
        "Save the cleaned DataFrame to a CSV file named 'df_clean.csv' without including the DataFrame index.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTjkeBZdNBsx",
        "outputId": "aff581ca-a46a-4b88-91fb-a03b98e60851"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "def load_and_filter_baseline_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads the ADNIMERGE data from a CSV file and selects a predefined set of\n",
        "    baseline columns.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the ADNIMERGE CSV file.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: A DataFrame containing only the baseline columns.\n",
        "    \"\"\"\n",
        "    # Handle potential mixed types based on previous warning\n",
        "    df = pd.read_csv(file_path, low_memory=False)\n",
        "\n",
        "    # Define the list of baseline columns to keep\n",
        "    baseline_cols = [\n",
        "        'RID', 'PTID', 'DX_bl', 'AGE', 'PTGENDER', 'PTEDUCAT', 'APOE4',\n",
        "        # Biomarkers\n",
        "        'ABETA_bl', 'TAU_bl', 'PTAU_bl', 'FDG_bl', 'PIB_bl', 'AV45_bl', 'FBB_bl',\n",
        "        # Clinical scores\n",
        "        'CDRSB_bl', 'ADAS11_bl', 'ADAS13_bl', 'ADASQ4_bl', 'MMSE_bl',\n",
        "        'RAVLT_immediate_bl', 'RAVLT_learning_bl', 'RAVLT_forgetting_bl', 'RAVLT_perc_forgetting_bl',\n",
        "        'LDELTOTAL_BL', 'DIGITSCOR_bl', 'TRABSCOR_bl', 'FAQ_bl',\n",
        "        'MOCA_bl', 'EcogPtMem_bl', 'EcogPtLang_bl', 'EcogPtVisspat_bl',\n",
        "        'EcogPtPlan_bl', 'EcogPtOrgan_bl', 'EcogPtDivatt_bl', 'EcogPtTotal_bl',\n",
        "        'EcogSPMem_bl', 'EcogSPLang_bl', 'EcogSPVisspat_bl', 'EcogSPPlan_bl',\n",
        "        'EcogSPOrgan_bl', 'EcogSPDivatt_bl', 'EcogSPTotal_bl',\n",
        "        # MRI volumes\n",
        "        'Ventricles_bl', 'Hippocampus_bl', 'WholeBrain_bl', 'Entorhinal_bl',\n",
        "        'Fusiform_bl', 'MidTemp_bl', 'ICV_bl'\n",
        "    ]\n",
        "\n",
        "    # Select only the baseline columns\n",
        "    df_bl = df[baseline_cols]\n",
        "\n",
        "    return df_bl\n",
        "\n",
        "def drop_irrelevant_and_high_missingness_columns(df):\n",
        "    \"\"\"\n",
        "    Drops specified Ecog score columns and high-missingness columns from the DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: The DataFrame with specified columns dropped.\n",
        "    \"\"\"\n",
        "    # Define columns to drop: Ecog scores\n",
        "    ecog_cols = [\n",
        "        'EcogPtMem_bl', 'EcogPtLang_bl', 'EcogPtVisspat_bl',\n",
        "        'EcogPtPlan_bl', 'EcogPtOrgan_bl', 'EcogPtDivatt_bl', 'EcogPtTotal_bl',\n",
        "        'EcogSPMem_bl', 'EcogSPLang_bl', 'EcogSPVisspat_bl', 'EcogSPPlan_bl',\n",
        "        'EcogSPOrgan_bl', 'EcogSPDivatt_bl', \"EcogSPTotal_bl\",\n",
        "    ]\n",
        "    # Define columns to drop: High missingness features identified previously (>50%)\n",
        "    high_missingness_cols = ['PIB_bl', 'FBB_bl', 'DIGITSCOR_bl', 'AV45_bl']\n",
        "\n",
        "    # Combine the lists of columns to drop\n",
        "    cols_to_drop = ecog_cols + high_missingness_cols\n",
        "\n",
        "    # Drop the specified columns. errors='ignore' prevents errors if a column is not found.\n",
        "    df_dropped = df.drop(columns=cols_to_drop, errors='ignore')\n",
        "\n",
        "    return df_dropped\n",
        "\n",
        "def deduplicate_by_ptid(df):\n",
        "    \"\"\"\n",
        "    Deduplicates the DataFrame based on the 'PTID' column, keeping the first\n",
        "    entry for each unique participant.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: The deduplicated DataFrame with reset index.\n",
        "    \"\"\"\n",
        "    # Remove exact duplicate rows first across all columns\n",
        "    df_nodup = df.drop_duplicates()\n",
        "\n",
        "    # Then deduplicate based on PTID, keeping the first entry for each participant\n",
        "    df_nodup = df_nodup.drop_duplicates(subset='PTID', keep='first')\n",
        "\n",
        "    # Reset the index of the resulting DataFrame for cleaner indexing\n",
        "    df_nodup.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return df_nodup\n",
        "\n",
        "def impute_missing_values(df):\n",
        "    \"\"\"\n",
        "    Imputes missing values in the DataFrame, applying mean imputation to\n",
        "    continuous variables and most frequent imputation to categorical variables.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): The input DataFrame with missing values.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: The DataFrame with missing values imputed.\n",
        "    \"\"\"\n",
        "    # Separate continuous and categorical columns based on previous analysis\n",
        "    # Note: ABETA_bl, TAU_bl, PTAU_bl are excluded as they have ~50% missingness and might require different handling\n",
        "    continuous_cols = [\n",
        "        'FDG_bl', 'MOCA_bl', 'WholeBrain_bl', 'Entorhinal_bl', 'Fusiform_bl',\n",
        "        'MidTemp_bl', 'ICV_bl', 'AGE','Ventricles_bl', 'Hippocampus_bl',\n",
        "        'PTEDUCAT', 'CDRSB_bl', 'ADAS11_bl', 'ADAS13_bl', 'ADASQ4_bl',\n",
        "        'MMSE_bl', 'RAVLT_immediate_bl', 'RAVLT_learning_bl',\n",
        "        'RAVLT_forgetting_bl', 'RAVLT_perc_forgetting_bl', 'LDELTOTAL_BL',\n",
        "        'TRABSCOR_bl', 'FAQ_bl', # MOCA_bl is listed twice, keep one\n",
        "    ]\n",
        "    # Ensure MOCA_bl is only listed once in continuous_cols\n",
        "    continuous_cols = list(set(continuous_cols))\n",
        "\n",
        "\n",
        "    categorical_cols = ['APOE4', 'DX_bl']\n",
        "\n",
        "    # Mean imputation for continuous variables\n",
        "    imp_mean = SimpleImputer(strategy='mean')\n",
        "    # Only impute if columns exist in the dataframe to prevent errors\n",
        "    existing_continuous_cols = [col for col in continuous_cols if col in df.columns]\n",
        "    if existing_continuous_cols:\n",
        "        df[existing_continuous_cols] = imp_mean.fit_transform(df[existing_continuous_cols])\n",
        "\n",
        "    # Most frequent imputation for categorical variables\n",
        "    imp_freq = SimpleImputer(strategy='most_frequent')\n",
        "    # Only impute if columns exist in the dataframe to prevent errors\n",
        "    existing_categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
        "    if existing_categorical_cols:\n",
        "        df[existing_categorical_cols] = imp_freq.fit_transform(df[existing_categorical_cols])\n",
        "\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the ADNIMERGE baseline data cleaning pipeline.\n",
        "\n",
        "    This function orchestrates the loading, filtering, dropping columns,\n",
        "    deduplication, and imputation steps.\n",
        "    \"\"\"\n",
        "    # Define input and output file paths\n",
        "    file_path = \"ADNIMERGE_25Apr2025.csv\"\n",
        "    output_file_path = \"df_clean.csv\"\n",
        "\n",
        "    # Step 1: Load data and filter for baseline columns\n",
        "    print(f\"Loading data from {file_path} and filtering for baseline...\")\n",
        "    df_bl = load_and_filter_baseline_data(file_path)\n",
        "    print(f\"Initial baseline data shape: {df_bl.shape}\")\n",
        "\n",
        "    # Step 2: Drop irrelevant and high-missingness columns\n",
        "    print(\"Dropping irrelevant and high-missingness columns...\")\n",
        "    df_dropped = drop_irrelevant_and_high_missingness_columns(df_bl)\n",
        "    print(f\"Shape after dropping columns: {df_dropped.shape}\")\n",
        "\n",
        "    # Step 3: Deduplicate data by PTID\n",
        "    print(\"Deduplicating data by PTID...\")\n",
        "    df_dedup = deduplicate_by_ptid(df_dropped)\n",
        "    print(f\"Shape after deduplication: {df_dedup.shape}\")\n",
        "    print(f\"Unique PTIDs after deduplication: {df_dedup['PTID'].nunique()}\")\n",
        "\n",
        "    # Step 4: Impute missing values\n",
        "    print(\"Imputing missing values...\")\n",
        "    # Create a copy before imputation to avoid modifying the intermediate DataFrame in place\n",
        "    df_clean = impute_missing_values(df_dedup.copy())\n",
        "    print(\"Missingness after imputation:\")\n",
        "    missing_percent_after_imputation = df_clean.isnull().mean() * 100\n",
        "    # Print only columns that still have missing values\n",
        "    print(missing_percent_after_imputation[missing_percent_after_imputation > 0])\n",
        "\n",
        "    # Step 5: Save the cleaned data to a CSV file\n",
        "    print(f\"Saving cleaned data to {output_file_path}...\")\n",
        "    df_clean.to_csv(output_file_path, index=False)\n",
        "    print(\"Data cleaning pipeline completed.\")\n",
        "\n",
        "# Ensure the main function runs only when the script is executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from ADNIMERGE_25Apr2025.csv and filtering for baseline...\n",
            "Initial baseline data shape: (16421, 49)\n",
            "Dropping irrelevant and high-missingness columns...\n",
            "Shape after dropping columns: (16421, 31)\n",
            "Deduplicating data by PTID...\n",
            "Shape after deduplication: (2430, 31)\n",
            "Unique PTIDs after deduplication: 2430\n",
            "Imputing missing values...\n",
            "Missingness after imputation:\n",
            "ABETA_bl    50.0\n",
            "TAU_bl      50.0\n",
            "PTAU_bl     50.0\n",
            "dtype: float64\n",
            "Saving cleaned data to df_clean.csv...\n",
            "Data cleaning pipeline completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "734dc661"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Python functions were successfully created for each step of the data cleaning pipeline: data loading and initial filtering, column dropping, deduplication, and missing value imputation.\n",
        "*   The `load_and_filter_baseline_data` function correctly loads the specified CSV and selects a predefined set of baseline columns.\n",
        "*   The `drop_irrelevant_and_high_missingness_columns` function effectively removes specified Ecog score columns and high-missingness columns (PIB\\_bl, FBB\\_bl, DIGITSCOR\\_bl, AV45\\_bl) from the DataFrame.\n",
        "*   The `deduplicate_by_ptid` function successfully removes exact duplicate rows and then deduplicates based on 'PTID', keeping the first entry.\n",
        "*   The `impute_missing_values` function applies mean imputation to a specified list of continuous columns and most frequent imputation to a specified list of categorical columns ('APOE4', 'DX\\_bl').\n",
        "*   A `main` function was created to orchestrate the entire data cleaning process by calling the refactored functions sequentially.\n",
        "*   The final cleaned DataFrame (`df_clean`) was successfully saved to a CSV file named 'df\\_clean.csv' with the index excluded.\n",
        "*   After imputation, the columns 'ABETA\\_bl', 'TAU\\_bl', and 'PTAU\\_bl' still show approximately 50% missingness, as they were intentionally excluded from the imputation lists based on prior analysis.\n",
        "*   Comments and docstrings were added to the functions and the main script to enhance code readability and reproducibility.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The refactored script provides a clean, modular, and reproducible pipeline for cleaning the ADNIMERGE baseline data, suitable for use in a GitHub repository.\n",
        "*   Further steps should address the handling of the remaining missing values in 'ABETA\\_bl', 'TAU\\_bl', and 'PTAU\\_bl', potentially using more advanced imputation techniques or considering their high missingness in subsequent analyses.\n"
      ]
    }
  ]
}